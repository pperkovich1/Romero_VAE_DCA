# submit file for VAEs in Pytorch
# TODO: make this consistent with other submit files

notification = Always
notify_user = jlwang5@wisc.edu

# Must set the universe to Docker
universe = docker
docker_image = romerolab/chtc:pytorch-nvidia-v16Dec19

# set the log, error and output files 
#
log = logs/chtc_model_training_$(Process)_$(dataset).log
error = logs/chtc_model_training_$(Process)_$(dataset).err
output = logs/chtc_model_training_$(Process)_$(dataset).out

# allows submit file to be run from anywhere?
# TODO: figure out while absolute filepaths don't work
# base = /home/jlwang5/VAEs/run
base = .
# dataset = processed_cmx_uniref100_90_80_10_100.fasta
squid = http://proxy.chtc.wisc.edu/SQUID/jlwang5

# set the executable to run
executable = $(base)/chtc_train_model.sh
arguments = $(Cluster)_$(Process) $(dataset) ../config.yaml

# take our python script and data to the compute node
# staging tarball contains entire github repository
transfer_input_files = $(base)/staging.tar.gz, $(base)/chtc_root.txt, $(base)/chtc_train_model.sh, $(base)/runmodel.sh,  $(squid)/$(dataset), $(squid)/vae_env.tar.gz

should_transfer_files = YES
when_to_transfer_output = ON_EXIT
  
# We must request 1 CPU in addition to 1 GPU (if needed)
request_cpus = 1

if ! defined DISABLE_GPU
# condor_submit was run from the command line like 
# condor_submit DISABLE_GPU=1 chtc_reweighting.sub
  request_gpus = 1
  # We require a machine with a modern version of the CUDA driver
  Requirements = (Target.CUDADriverVersion >= 10.1)

  +WantGPULab = true
  +GPUJobLength = "short"

endif

# select some memory and disk space
request_memory = 5GB
request_disk = 5GB

if ! defined DISABLE_FLOCKING
    +WantFlocking = false
endif

# Tell HTCondor to run 1 instances of our job:
queue dataset from datasets.txt
