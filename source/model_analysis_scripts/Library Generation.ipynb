{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method 1: Encode library into latent space\n",
    "Method 2: Generate library from latent vector (options: which vector to use)\n",
    "Method 3: Analyze library statistics (diversity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = ['../working/cmx_latent.pkl', '../working/origin_latent.pkl', '../working/wt_latent.pkl']\n",
    "\n",
    "\n",
    "# code for plotting training, origin, and WT\n",
    "# A MESS\n",
    "# FOR SOME REASON THE FIRST FILE HAS DIFFERENT KEYS, I THINK BECAUSE I RAN SAMEER'S CODE?\n",
    "# flag = True\n",
    "for i, path in enumerate(paths):\n",
    "#     if flag:\n",
    "#         flag = False\n",
    "#         file = open(path, 'rb')\n",
    "#         data = pickle.load(file)\n",
    "#     #     print(data.keys())\n",
    "#         means = data['background_latent']\n",
    "#     #     means = [foo[0].tolist() for foo in latents]\n",
    "#         xs = [mean[0] for mean in means]\n",
    "#         ys = [mean[1] for mean in means]\n",
    "#     else:\n",
    "    file = open(path, 'rb')\n",
    "    data = pickle.load(file)\n",
    "#     print(data.keys())\n",
    "    latents = data['latent']\n",
    "    means = [foo[0].tolist() for foo in latents]\n",
    "    xs = [mean[0] for mean in means]\n",
    "    ys = [mean[1] for mean in means]\n",
    "\n",
    "    if i == 2:\n",
    "        plt.scatter(xs, ys, s=.5, color='red')\n",
    "    else:\n",
    "        plt.scatter(xs, ys, s=.5)\n",
    "\n",
    "# code to plot repeated 2d training set\n",
    "# for i in range(5):\n",
    "#     path = '../working/2d_%d.pkl'%i\n",
    "#     file = open(path, 'rb')\n",
    "#     data = pickle.load(file)\n",
    "# #     print(data.keys())\n",
    "#     latents = data['latent']\n",
    "#     means = [foo[0].tolist() for foo in latents]\n",
    "#     xs = [mean[0] for mean in means]\n",
    "#     ys = [mean[1] for mean in means]\n",
    "\n",
    "#     plt.scatter(xs, ys, s=.5)    \n",
    "\n",
    "plt.legend(['Training set', 'Generated from origin', 'Generated from WT'])\n",
    "plt.xlabel('Latent Dim 1')\n",
    "plt.ylabel('Latent Dim 2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config('../config_1000_15.yaml')\n",
    "dataset = utils.get_dataset_from_config(config)\n",
    "seqs = list(dataset)\n",
    "seqs = torch.stack([row[0] for row in seqs])\n",
    "seqs = seqs.reshape(seqs.size()[0], -1, 21)\n",
    "model = utils.load_model_from_config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent = pickle.load(open(config.latent_fullpath, 'rb'))['latent']\n",
    "means = [row[0] for row in latent]\n",
    "log_vars = [row[1] for row in latent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from_vector(means, log_vars, sample_size):\n",
    "    with torch.no_grad():\n",
    "        outputs = []\n",
    "        for i in range(sample_size):\n",
    "            encoding = model.reparameterize(means, log_vars)\n",
    "            output = model.decoder(encoding)\n",
    "            outputs.append(output)\n",
    "        outputs = torch.stack(outputs)\n",
    "        recons = utils.softmax(outputs)\n",
    "        # before reshape: sample_size x one_hot_encoding\n",
    "        recons = recons.reshape(sample_size, -1, 21)\n",
    "        # after reshape: sample_size x seq_length x amino_acids\n",
    "    return recons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_latent_vector(means, log_vars, sample_size=100):\n",
    "    all_recons = {}\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # all variation (normal sampling)\n",
    "        # torch.stack([]) makes this a 2D tensor\n",
    "        free_recons = sample_from_vector(means, log_vars, sample_size)\n",
    "        all_recons['free'] = free_recons.reshape(1, *free_recons.size())\n",
    "\n",
    "        # all fixed (no variation)\n",
    "        fixed_outputs = []\n",
    "        fixed_log_vars = torch.full(log_vars.size(), -float('inf')) # bc e^(-inf)=0\n",
    "        fixed_recons = sample_from_vector(means, fixed_log_vars, sample_size)\n",
    "        all_recons['fixed'] = fixed_recons.reshape(1, *fixed_recons.size())\n",
    "\n",
    "        # one variation, others fixed\n",
    "        single_free = []\n",
    "        for free_var in range(len(log_vars)):\n",
    "            single_free_log_vars = torch.full(log_vars.size(), -float('inf'))\n",
    "            single_free_log_vars[free_var] = log_vars[free_var]\n",
    "            single_free.append(sample_from_vector(means, single_free_log_vars, sample_size))\n",
    "        all_recons['one_free']=torch.stack(single_free)\n",
    "        \n",
    "        # one fixed, others variation\n",
    "        single_fixed = []\n",
    "        for fixed_var in range(len(log_vars)):\n",
    "            single_fixed_log_vars = log_vars.clone()\n",
    "            single_fixed_log_vars[fixed_var] = -float('inf')\n",
    "            single_fixed.append(sample_from_vector(means, single_fixed_log_vars, sample_size))\n",
    "        all_recons['one_fixed'] = torch.stack(single_fixed)\n",
    "            \n",
    "    return all_recons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_latent_origin(latent_size, sample_size=100):\n",
    "    all_recons = {}\n",
    "    # mean 0 and variance 1\n",
    "    means = torch.zeros(latent_size)\n",
    "    log_vars = torch.ones(latent_size)\n",
    "    return explore_latent_vector(means, log_vars, sample_size=sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "def freqs_to_heatmap(freqs, adjustment=None, label='adjustment', xlabel=None):\n",
    "    %matplotlib inline\n",
    "    \n",
    "    to_plot = []\n",
    "    adjusted = freqs\n",
    "    if not adjustment is None:\n",
    "        adjusted = freqs-adjustment\n",
    "        to_plot.extend(adjustment)\n",
    "    to_plot.extend(adjusted)\n",
    "    to_plot = torch.stack(to_plot)\n",
    "    \n",
    "    # current state of matrix: [#latents+stuff x #positions x amino_acids ]\n",
    "    plottable = to_plot.transpose(1, 2)\n",
    "    # current state of matrix: [#latents+stuff x amino_acids x #positions]\n",
    "    plottable = plottable.reshape(-1, plottable.size()[-1])\n",
    "    # current state of matrix: [(#latents*21 amino acids) x #positions]\n",
    "\n",
    "\n",
    "    figure = plt.figure(figsize=(15,15), dpi=250)\n",
    "    ax = plt.axes()\n",
    "    ax.set_yticks(range(0, len(plottable), 21))\n",
    "    ax.set_yticklabels([label, *range(0, len(plottable)-2)])\n",
    "    if not xlabel==None:\n",
    "        ax.set_xticks(range(0, len(plottable[0])))\n",
    "        ax.set_xticklabels(ss_dataset,fontsize=3, color='red')\n",
    "    ax.tick_params(colors='red')\n",
    "    heatmap = ax.imshow(plottable)\n",
    "    divider = make_axes_locatable(ax)\n",
    "    \n",
    "    ax.set_ylabel('Variation type', color='red')\n",
    "    ax.set_xlabel('Position', color='red')\n",
    "\n",
    "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "    cbar = figure.colorbar(heatmap, cax=cax)\n",
    "    cbar.ax.tick_params(colors='red')\n",
    "    \n",
    "    return figure\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_wt = seqs[0]\n",
    "trimmed_wt = [aa for aa in input_wt if not aa[-1]==0]\n",
    "print(len(trimmed_wt))\n",
    "print(len(wt_ss))\n",
    "\n",
    "# input_3vdh = seqs[-1]\n",
    "aligned_ss = []\n",
    "i = 0\n",
    "for aa in input_wt:\n",
    "    if not aa[-1] == 0:\n",
    "        aligned_ss.append(wt_ss[i][1])\n",
    "        i+=1\n",
    "    else:\n",
    "        aligned_ss.append('-')\n",
    "i = None\n",
    "\n",
    "latent_wt = (means[0], log_vars[0])\n",
    "\n",
    "# these are all dictionaries of various latent sampling for different starting sequences\n",
    "recons_wt = explore_latent_vector(means[0], log_vars[0], sample_size=10)\n",
    "# recons_3vdh = explore_latent_vector(means[-1], log_vars[-1], sample_size=10)\n",
    "recons_o = explore_latent_origin(len(means[0]), sample_size = 10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a plot of structure vs sequence for WT\n",
    "\n",
    "figure = plt.figure(figsize=(15,15), dpi=250)\n",
    "ax = plt.axes()\n",
    "ax.imshow(input_wt.transpose(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs_wt = examine_model.seqs_to_freqs_dict(recons_wt)\n",
    "# freqs_3vdh = examine_model.seqs_to_freqs_dict(recons_3vdh)\n",
    "freqs_o = examine_model.seqs_to_freqs_dict(recons_o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_free_wt_fig = freqs_to_heatmap(freqs_wt['one_free'], adjustment=freqs_wt['fixed'],\n",
    "                                   label='no variation', xlabel=ss_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_fixed_wt_fig = freqs_to_heatmap(freqs_wt['one_fixed'], adjustment=freqs_wt['free'], label='all variation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freqs_to_diversity(freqs_dict):\n",
    "    # input tensor dimensions: latent_group x position x amino_acids\n",
    "    # this is Shannon's index. IDK if it's a good metric, though\n",
    "    divs = {}\n",
    "    for key, freqs in freqs_dict.items():\n",
    "        print(key)\n",
    "        divs[key] = torch.tensor([[-torch.sum(pos[~(pos==0)]*torch.log(pos[~(pos==0)])) for pos in seq] for seq in freqs])\n",
    "    return divs\n",
    "\n",
    "def plot_diversity(div):\n",
    "    fig = plt.figure(figsize=(30, 30), dpi=250)\n",
    "    ax = plt.axes()\n",
    "    ax.set_yticks(range(0,len(div)))\n",
    "    ax.set_yticklabels(['Free', 'Fixed', *range(0, len(div)-2)], color='red')\n",
    "    \n",
    "    ax.tick_params(axis='y', labelsize=3)\n",
    "    heatmap = ax.imshow(div)\n",
    "    divider = make_axes_locatable(ax)\n",
    "\n",
    "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "    cbar = fig.colorbar(heatmap, cax=cax)\n",
    "    cbar.ax.tick_params(colors='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "divs_wt = freqs_to_diversity(freqs_wt)\n",
    "# here I change divs_wt to a list for plotting\n",
    "# it is jank. I will fix later\n",
    "plottable = []\n",
    "for val in divs_wt.values():\n",
    "    for div in val:\n",
    "        plottable.append(div)\n",
    "\n",
    "plottable = torch.stack(plottable)\n",
    "divs_wt_fig = plot_diversity(plottable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting WT diversity to 3vdh diversity\n",
    "## Uses sampling based on WT seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wt seq aligned with dataset\n",
    "wt_oh_gaps = input_wt.reshape(1, *input_wt.size())\n",
    "wt_seq_gaps = utils.one_hots_to_proteins(wt_oh_gaps)\n",
    "wt_divs_gaps = torch.cat(list(divs_wt.values()))\n",
    "labels = {'free':slice(0,1), 'fixed':slice(1,2), 'one_free':slice(2,17), 'one_fixed':slice(17,32)}\n",
    "\n",
    "# wt seq with no gaps\n",
    "wt_seq_divs = utils.remove_gaps(wt_seq_gaps, wt_divs_gaps)\n",
    "wt_seq = wt_seq_divs[0]\n",
    "wt_divs = wt_seq_divs[1:]\n",
    "\n",
    "# wt seq aligned with 3vdh\n",
    "wt_seq_align = np.array(wt_aln) # taken from cell 3\n",
    "wt_divs_align = np.full((len(wt_divs), len(wt_seq_align)), np.nan)\n",
    "base_i = 0\n",
    "for (align_i, aa) in enumerate(wt_seq_align):\n",
    "    if aa == '-':\n",
    "        pass\n",
    "    elif wt_seq[base_i] == aa:\n",
    "        wt_divs_align[:, align_i] = wt_divs[:, base_i]\n",
    "        base_i += 1\n",
    "    else:\n",
    "        print('uh oh the sequences don\\'t match!')\n",
    "        print(base_i, align_i)\n",
    "        print(wt_seq[base_i], aa)\n",
    "        break\n",
    "i = None\n",
    "\n",
    "# final product: wt_divs_align"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
